# -*- coding: utf-8 -*-
"""MNISTtoy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kQL8j5zLxihDqE_FyPaqDFJPl-yzD0vh
"""

import torch
import numpy as np
import torchvision as tv
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision.utils import save_image
from torchvision.datasets import MNIST
from torch.utils.data.dataloader import default_collate
from torch.utils.data import ConcatDataset
import os

if not os.path.exists('./vae_img'):
    os.mkdir('./vae_img')

if not os.path.exists('./vae_img_pred'):
    os.mkdir('./vae_img_pred')


def to_img(x):
    x = x.clamp(0, 1)
    x = x.view(x.size(0), 1, 28, 28)
    return x


num_epochs = 101
batch_size = 128
learning_rate = 1e-3
pretrained_VAE = False

img_transform = transforms.Compose([
    transforms.ToTensor()
    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

def my_collate(batch):
    modified_batch = []
    for item in batch:
        image, label = item
        if label == 0:
            modified_batch.append(item)
    return default_collate(modified_batch)


#dataset = MNIST('./data', transform=img_transform, download=True, train=True)
#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
def weights_init(m):
    if isinstance(m, nn.Conv2d):
        torch.nn.init.xavier_uniform(m.weight.data)

num_datapoints = 5000

#dataloaderzeros = DataLoader(
#    MNIST('./data', train=True, download=True, transform=img_transform),
#    batch_size=batch_size, shuffle=True, collate_fn = my_collate)
print('Making each number training set')
datasets = []
for i in range(10):
  dataset = MNIST('./data', transform=img_transform, download=True, train = True)
  #print(dataset)
  #[0:5851]
  idx = dataset.targets==i
  dataset.targets = dataset.targets[idx]
  dataset.data = dataset.data[idx]
  dataset = torch.utils.data.random_split(dataset, [num_datapoints, len(dataset)-num_datapoints])[0]
  #dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
  datasets.append(dataset)
  #print(len(dataset))

print('Making each number combined with 0 by percent predicter set')
VAE_dataloaders_w_zeros = []
for i in range(10):
  numbersets = []
  for j in range(11):
      numberzeros = int(num_datapoints * (10-j) /10)
      numbervals = int(num_datapoints * j / 10)
      #print(numberzeros)
      #print(numbervals)
      datasetandzero = [torch.utils.data.random_split(datasets[0], [numberzeros, len(dataset)-numberzeros])[0], 
                        torch.utils.data.random_split(datasets[i], [numbervals, len(dataset)-numbervals])[0]]
      combined = ConcatDataset(datasetandzero)
      dataloader = DataLoader(combined, batch_size=batch_size, shuffle=True)
      numbersets.append(dataloader)
  VAE_dataloaders_w_zeros.append(numbersets)

print(len(VAE_dataloaders_w_zeros))
print(len(VAE_dataloaders_w_zeros[0]))




print('Making each number test set')
test_data_loaders = []
for i in range(10):
  dataset = MNIST('./data', transform=img_transform, download=True, train = False)
  idx = dataset.targets==i
  dataset.targets = dataset.targets[idx]
  dataset.data = dataset.data[idx]
  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
  test_data_loaders.append(dataloader)

class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()

        self.fc1 = nn.Linear(784, 400)
        self.fc21 = nn.Linear(400, 20)
        self.fc22 = nn.Linear(400, 20)
        self.fc3 = nn.Linear(20, 400)
        self.fc4 = nn.Linear(400, 784)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparametrize(self, mu, logvar):
        std = logvar.mul(0.5).exp_()
        if torch.cuda.is_available():
            eps = torch.cuda.FloatTensor(std.size()).normal_()
        else:
            eps = torch.FloatTensor(std.size()).normal_()
        eps = Variable(eps)
        return eps.mul(std).add_(mu)

    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return F.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparametrize(mu, logvar)
        return self.decode(z), mu, logvar


model = VAE()
if torch.cuda.is_available():
    model.cuda()

reconstruction_function = nn.MSELoss(size_average=False)


def loss_function(recon_x, x, mu, logvar):
    """
    recon_x: generating images
    x: origin images
    mu: latent mean
    logvar: latent log variance
    """
    BCE = reconstruction_function(recon_x, x)  # mse loss
    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)
    KLD = torch.sum(KLD_element).mul_(-0.5)
    # KL divergence
    return BCE + KLD



"""
if pretrained_VAE and os.path.exists('./vae.pth'):
    model.load_state_dict(torch.load( './vae.pth'))
else:
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for batch_idx, data in enumerate(dataloader):
            img, _ = data
            img = img.view(img.size(0), -1)
            img = Variable(img)
            if torch.cuda.is_available():
                img = img.cuda()
            optimizer.zero_grad()
            recon_batch, mu, logvar = model(img)
            loss = loss_function(recon_batch, img, mu, logvar)
            loss.backward()
            train_loss += loss.item()
            optimizer.step()
            if batch_idx % 100 == 0:
                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                    epoch,
                    batch_idx * len(img),
                    len(dataloader.dataset), 100. * batch_idx / len(dataloader),
                    loss.item() / len(img)))

        print('====> Epoch: {} Average loss: {:.4f}'.format(
            epoch, train_loss / len(dataloader.dataset)))
        if epoch % 10 == 0:
            save = to_img(recon_batch.cpu().data)
            save_image(save, './vae_img/image_{}.png'.format(epoch))
    model.train(mode=False)
    torch.save(model.state_dict(), './vae.pth')

for epoch in range(num_epochs):
    predModel.train()
    train_loss = 0
    for batch_idx, data in enumerate(dataloaderzeros):
        img, _ = data
        img = img.view(img.size(0), -1)
        img = Variable(img)
        if torch.cuda.is_available():
            img = img.cuda()
        optimizer.zero_grad()
        mu, logvar = model.encode(img)
        modelOutput = model.reparametrize(mu, logvar)
        
        predOutput = predModel(img)
        recon_batch = model.decode(predOutput)
        recon_batch_truth, mu, logvar = model(img)
        predLoss = pred_loss_function(predOutput, modelOutput)
        predLoss.backward()
        train_loss += predLoss.item()
        optimizer.step()
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch,
                batch_idx * len(img),
                len(dataloaderzeros.dataset), 100. * batch_idx / len(dataloaderzeros),
                predLoss.item() / len(img)))

    print('====> Epoch: {} Average loss: {:.4f}'.format(
        epoch, train_loss / len(dataloaderzeros.dataset)))
    if epoch % 10 == 0:
        save = to_img(recon_batch.cpu().data)
        save_image(save, './vae_img_pred/image_{}.png'.format(epoch))
        save_truth = to_img(recon_batch_truth.cpu().data)
        save_image(save_truth, './vae_img_pred/image_{}_truth.png'.format(epoch))
predmodel.train(mode=False)
torch.save(predModel.state_dict(), './vaePred.pth')

train_loss = 0
dataloader = test_data_loaders[1]
for batch_idx, data in enumerate(dataloader):
    img, _ = data
    img = img.view(img.size(0), -1)
    img = Variable(img)
    if torch.cuda.is_available():
        img = img.cuda()
    optimizer.zero_grad()
    mu, logvar = model.encode(img)
    modelOutput = model.reparametrize(mu, logvar)
    
    predOutput = predModel(img)
    #recon_batch = model.decode(predOutput)
    #recon_batch_truth, mu, logvar = model(img)
    predLoss = pred_loss_function(predOutput, modelOutput)
    train_loss += predLoss.item()
    if batch_idx % 100 == 0:
        print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
            epoch,
            batch_idx * len(img),
            len(dataloader.dataset), 100. * batch_idx / len(dataloader),
            predLoss.item() / len(img)))

print('====> Average loss: {:.4f}'.format(
    train_loss / len(dataloader.dataset)))
"""

#full system
loss_scores = np.zeros((10, 11))

torch.save(model.state_dict(), './vae_random.pth')

#loop through each number
for number in range(1,10):
    #loop through each number
    for percent in range(11):
        print('Training VAE for number {} and percent {}'.format(number, percent))
        model.load_state_dict(torch.load( './vae_random.pth'))
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
        model.train()
        print('new weights')
        for epoch in range(num_epochs):
            train_loss = 0
            for batch_idx, data in enumerate(VAE_dataloaders_w_zeros[number][percent]):
                img, _ = data
                img = img.view(img.size(0), -1)
                img = Variable(img)
                if torch.cuda.is_available():
                    img = img.cuda()
                optimizer.zero_grad()
                recon_batch, mu, logvar = model(img)
                loss = loss_function(recon_batch, img, mu, logvar)
                loss.backward()
                train_loss += loss.item()
                optimizer.step()
                #if batch_idx % 100 == 0:
                #    print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                #        epoch,
                #        batch_idx * len(img),
                #        len(dataloader.dataset), 100. * batch_idx / len(dataloader),
                #        loss.item() / len(img)))
            
            if epoch % 10 == 0:
                print('====> Epoch: {} Average loss: {:.4f}'.format(
                    epoch, train_loss / len(VAE_dataloaders_w_zeros[number][percent].dataset)))
                save = to_img(recon_batch.cpu().data)
                save_image(save, './vae_img/image_{}.png'.format(epoch))
        torch.save(model.state_dict(), './vae_{}_{}.pth'.format(number, percent))
       



        print('Testing VAE for number {} and percent {}'.format(number,percent))
        #test predictor on all new numbers
        test_loss = 0
        dataloader = test_data_loaders[number]
        for batch_idx, data in enumerate(dataloader):
            img, _ = data
            img = img.view(img.size(0), -1)
            img = Variable(img)
            if torch.cuda.is_available():
                img = img.cuda() 
            recon_batch, mu, logvar = model(img)
            loss = loss_function(recon_batch, img, mu, logvar)
            test_loss += loss.item()
            #if batch_idx % 100 == 0:
            #    print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
            #        epoch,
            #        batch_idx * len(img),
            #        len(dataloader.dataset), 100. * batch_idx / len(dataloader),
            #        predLoss.item() / len(img)))

        print('====> Average Test loss: {:.4f}'.format(
            test_loss / len(dataloader.dataset)))
        save = to_img(recon_batch.cpu().data)
        save_image(save, './vae_img/network_{}_percent_{}.png'.format(number, percent))
        save_image(img, './vae_img_pred/network_{}_percent_{}_truth.png'.format(number, percent))
        loss_scores[number,percent] = test_loss / len(dataloader.dataset)

    

    #store in vector
print(loss_scores)
np.save('loss_scores.npy', loss_scores)

print(loss_scores)
